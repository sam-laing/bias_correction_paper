@ARTICLE{NGdescent,
  author={Amari, Shun-ichi},
  journal={Neural Computation}, 
  title={Natural Gradient Works Efficiently in Learning}, 
  year={1998},
  volume={10},
  number={2},
  pages={251-276},
  keywords={},
  doi={10.1162/089976698300017746}
}
@article{RobbinsMonro1951,
  author  = {Robbins, Herbert and Monro, Sutton},
  title   = {A stochastic approximation method},
  journal = {Annals of Mathematical Statistics},
  volume  = {22},
  number  = {3},
  pages   = {400--407},
  year    = {1951}
}
@misc{orvieto2025searchadamssecretsauce,
      title={In Search of Adam's Secret Sauce}, 
      author={Antonio Orvieto and Robert Gower},
      year={2025},
      eprint={2505.21829},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.21829}, 
}
@online{hinton_rmsprop_2012,
  author    = {Geoffrey Hinton and Nitish Srivastava and Kevin Swersky},
  title     = {Neural Networks for Machine Learning -- Lecture 6e: RMSprop},
  year      = {2012},
  institution = {University of Toronto},
  url       = {http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
}
@misc{pagliardini2024ademamixoptimizerbetterfaster,
      title={The AdEMAMix Optimizer: Better, Faster, Older}, 
      author={Matteo Pagliardini and Pierre Ablin and David Grangier},
      year={2024},
      eprint={2409.03137},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.03137}, 
}
@misc{zhang2019rootmeansquarelayer,
      title={Root Mean Square Layer Normalization}, 
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.07467}, 
}
@misc{reddi2019convergenceadam,
      title={On the Convergence of Adam and Beyond}, 
      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
      year={2019},
      eprint={1904.09237},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.09237}, 
}
@misc{shen2024slimpajamadcunderstandingdatacombinations,
      title={SlimPajama-DC: Understanding Data Combinations for LLM Training}, 
      author={Zhiqiang Shen and Tianhua Tao and Liqun Ma and Willie Neiswanger and Zhengzhong Liu and Hongyi Wang and Bowen Tan and Joel Hestness and Natalia Vassilieva and Daria Soboleva and Eric Xing},
      year={2024},
      eprint={2309.10818},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.10818}, 
}
@misc{weber2024redpajamaopendatasettraining,
      title={RedPajama: an Open Dataset for Training Large Language Models}, 
      author={Maurice Weber and Daniel Fu and Quentin Anthony and Yonatan Oren and Shane Adams and Anton Alexandrov and Xiaozhong Lyu and Huu Nguyen and Xiaozhe Yao and Virginia Adams and Ben Athiwaratkun and Rahul Chalamala and Kezhen Chen and Max Ryabinin and Tri Dao and Percy Liang and Christopher Ré and Irina Rish and Ce Zhang},
      year={2024},
      eprint={2411.12372},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.12372}, 
}
@misc{keskar2017improvinggeneralizationperformanceswitching,
      title={Improving Generalization Performance by Switching from Adam to SGD}, 
      author={Nitish Shirish Keskar and Richard Socher},
      year={2017},
      eprint={1712.07628},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.07628}, 
}
@misc{wilson2018marginalvalueadaptivegradient,
      title={The Marginal Value of Adaptive Gradient Methods in Machine Learning}, 
      author={Ashia C. Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and Benjamin Recht},
      year={2018},
      eprint={1705.08292},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1705.08292}, 
}
@misc{su2023roformerenhancedtransformerrotary,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}
@misc{bernstein2024oldoptimizernewnorm,
      title={Old Optimizer, New Norm: An Anthology}, 
      author={Jeremy Bernstein and Laker Newhouse},
      year={2024},
      eprint={2409.20325},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.20325}, 
}
 @misc{Karpathy2022,
  author = {Andrej Karpathy},
  title = {\text{NanoGPT}},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}},
  commit = {325be85d9be8c81b436728a420e85796c57dba7e}
}
@misc{yuan2020eadamoptimizerepsilonimpact,
      title={EAdam Optimizer: How $\epsilon$ Impact Adam}, 
      author={Wei Yuan and Kai-Xin Gao},
      year={2020},
      eprint={2011.02150},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.02150}, 
}


@misc{gupta2018shampoopreconditionedstochastictensor,
      title={Shampoo: Preconditioned Stochastic Tensor Optimization}, 
      author={Vineet Gupta and Tomer Koren and Yoram Singer},
      year={2018},
      eprint={1802.09568},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.09568}, 
}}

@misc{pethick2025trainingdeeplearningmodels,
      title={Training Deep Learning Models with Norm-Constrained LMOs}, 
      author={Thomas Pethick and Wanyun Xie and Kimon Antonakopoulos and Zhenyu Zhu and Antonio Silveti-Falls and Volkan Cevher},
      year={2025},
      eprint={2502.07529},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.07529}, 
}
@misc{balles2020dissectingadamsignmagnitude,
      title={Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients}, 
      author={Lukas Balles and Philipp Hennig},
      year={2020},
      eprint={1705.07774},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1705.07774}, 
}
@misc{bernstein2025deriving,
  author = {Jeremy Bernstein},
  title = {Deriving Muon},
  url = {https://jeremybernste.in/writing/deriving-muon},
  year = {2025}
}
@misc{li2023convergenceadamrelaxedassumptions,
      title={Convergence of Adam Under Relaxed Assumptions}, 
      author={Haochuan Li and Alexander Rakhlin and Ali Jadbabaie},
      year={2023},
      eprint={2304.13972},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2304.13972}, 
}

@misc{moonshot,
      title={Muon is Scalable for LLM Training}, 
      author={Jingyuan Liu and Jianlin Su and Xingcheng Yao and Zhejun Jiang and Guokun Lai and Yulun Du and Yidao Qin and Weixin Xu and Enzhe Lu and Junjie Yan and Yanru Chen and Huabin Zheng and Yibo Liu and Shaowei Liu and Bohong Yin and Weiran He and Han Zhu and Yuzhi Wang and Jianzhou Wang and Mengnan Dong and Zheng Zhang and Yongsheng Kang and Hao Zhang and Xinran Xu and Yutao Zhang and Yuxin Wu and Xinyu Zhou and Zhilin Yang},
      year={2025},
      eprint={2502.16982},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.16982}, 
}

@misc{jordan2024muon,
  author       = {Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and
                  Franz Cesista and Laker Newhouse and Jeremy Bernstein},
  title        = {Muon: An optimizer for hidden layers in neural networks},
  year         = {2024},
  url          = {https://kellerjordan.github.io/posts/muon/}
}

@misc{Idelbayev18a,
  author       = "Yerlan Idelbayev",
  title        = "Proper {ResNet} Implementation for {CIFAR10/CIFAR100} in {PyTorch}",
  howpublished = "\url{https://github.com/akamaster/pytorch_resnet_cifar10}",
  note         = "Accessed: 20xx-xx-xx"
}
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  publisher={University of Toronto}
}


@article{swiGLU,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05202},
  eprinttype    = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{bernstein2024modulardualitydeeplearning,
      title={Modular Duality in Deep Learning}, 
      author={Jeremy Bernstein and Laker Newhouse},
      year={2024},
      eprint={2410.21265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.21265}, 
}
@inbook{RiedmillerBraun1993_247593,
    author       = {Riedmiller, Martin and Braun, Heinrich},
    year         = {1993},
    title        = {RPROP - a fast adaptive learning algorithm},
    language     = {english},
    booktitle    = {In: Proceedings of the 7th International Symposium of Computer and Information Science (ISCIS), Antalya, Turkey 1992. S. 279-286.}
}
@book{Goodfellow-et-al-2016,
    title = {Deep Learning},
    author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher = {MIT Press},
    year = {2016},
    note = {\url{http://www.deeplearningbook.org}},
    isbn = {978-0262035613}
}
@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}
@misc{huang2018denselyconnectedconvolutionalnetworks,
      title={Densely Connected Convolutional Networks}, 
      author={Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
      year={2018},
      eprint={1608.06993},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1608.06993}, 
}
@misc{zhang2024transformersneedadamhessian,
      title={Why Transformers Need Adam: A Hessian Perspective}, 
      author={Yushun Zhang and Congliang Chen and Tian Ding and Ziniu Li and Ruoyu Sun and Zhi-Quan Luo},
      year={2024},
      eprint={2402.16788},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.16788}, 
}
@misc{zhang2024transformersneedadamhessian,
      title={Why Transformers Need Adam: A Hessian Perspective}, 
      author={Yushun Zhang and Congliang Chen and Tian Ding and Ziniu Li and Ruoyu Sun and Zhi-Quan Luo},
      year={2024},
      eprint={2402.16788},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.16788}, 
}
@inproceedings{
sreckovic2025is,
title={Is your batch size the problem? Revisiting the Adam-{SGD} gap in language modeling},
author={Teodora Sre{\'c}kovi{\'c} and Jonas Geiping and Antonio Orvieto},
booktitle={High-dimensional Learning Dynamics 2025},
year={2025},
url={https://openreview.net/forum?id=R5mcjzbQa6}
}
@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}
@misc{cubuk2019autoaugmentlearningaugmentationpolicies,
      title={AutoAugment: Learning Augmentation Policies from Data}, 
      author={Ekin D. Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V. Le},
      year={2019},
      eprint={1805.09501},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1805.09501}, 
}
@misc{modded_nanogpt_2024,
  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and
                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and
                  Franz Cesista and Braden Koszarsky and @Grad62304977},
  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},
  year         = {2024},
  url          = {https://github.com/KellerJordan/modded-nanogpt}
}
@misc{jordan202494cifar10329seconds,
      title={94% on CIFAR-10 in 3.29 Seconds on a Single GPU}, 
      author={Keller Jordan},
      year={2024},
      eprint={2404.00498},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.00498}, 
}
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}
@misc{paszke2019pytorchimperativestylehighperformance,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01703}, 
}

@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}
@misc{defossez2022simpleconvergenceproofadam,
      title={A Simple Convergence Proof of Adam and Adagrad}, 
      author={Alexandre Défossez and Léon Bottou and Francis Bach and Nicolas Usunier},
      year={2022},
      eprint={2003.02395},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2003.02395}, 
}
@misc{loshchilov2017sgdrstochasticgradientdescent,
      title={SGDR: Stochastic Gradient Descent with Warm Restarts}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2017},
      eprint={1608.03983},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1608.03983}, 
}
@misc{ajroldi2024plainlm,
  author = {Niccolò Ajroldi},
  title = {plainLM: Language Model Pretraining in PyTorch},
  year = {2024},
  howpublished = {\url{https://github.com/Niccolo-Ajroldi/plainLM}}
}
@misc{biderman2023pythiasuiteanalyzinglarge,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01373}, 
}
@misc{black2022gptneox20bopensourceautoregressivelanguage,
      title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model}, 
      author={Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach},
      year={2022},
      eprint={2204.06745},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.06745}, 
}
@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}
@misc{taniguchi2024adoptmodifiedadamconverge,
      title={ADOPT: Modified Adam Can Converge with Any $\beta_2$ with the Optimal Rate}, 
      author={Shohei Taniguchi and Keno Harada and Gouki Minegishi and Yuta Oshima and Seong Cheol Jeong and Go Nagahara and Tomoshi Iiyama and Masahiro Suzuki and Yusuke Iwasawa and Yutaka Matsuo},
      year={2024},
      eprint={2411.02853},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.02853}, 
}
@misc{john2021adamdimprovedbiascorrectionadam,
      title={AdamD: Improved bias-correction in Adam}, 
      author={John St John},
      year={2021},
      eprint={2110.10828},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.10828}, 
}
@misc{xiao2025rethinkingconventionalwisdommachine,
      title={Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling}, 
      author={Lechao Xiao},
      year={2025},
      eprint={2409.15156},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.15156}, 
}
@misc{wortsman2023smallscaleproxieslargescaletransformer,
      title={Small-scale proxies for large-scale Transformer training instabilities}, 
      author={Mitchell Wortsman and Peter J. Liu and Lechao Xiao and Katie Everett and Alex Alemi and Ben Adlam and John D. Co-Reyes and Izzeddin Gur and Abhishek Kumar and Roman Novak and Jeffrey Pennington and Jascha Sohl-dickstein and Kelvin Xu and Jaehoon Lee and Justin Gilmer and Simon Kornblith},
      year={2023},
      eprint={2309.14322},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.14322}, 
}
@misc{yun2025stabilizingbackpropagation16bitneural,
      title={Stabilizing Backpropagation in 16-bit Neural Training with Modified Adam Optimizer}, 
      author={Juyoung Yun},
      year={2025},
      eprint={2307.16189},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.16189}, 
}
@misc{zhang2020adaptivemethodsgoodattention,
      title={Why are Adaptive Methods Good for Attention Models?}, 
      author={Jingzhao Zhang and Sai Praneeth Karimireddy and Andreas Veit and Seungyeon Kim and Sashank J Reddi and Sanjiv Kumar and Suvrit Sra},
      year={2020},
      eprint={1912.03194},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1912.03194}, 
}


@misc{hwang2024fadamadamnaturalgradient,
      title={FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information}, 
      author={Dongseong Hwang},
      year={2024},
      eprint={2405.12807},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.12807}, 
}

@article{polyak1964speeding,
  author       = {Boris T. Polyak},
  title        = {Some methods of speeding up the convergence of iteration methods},
  journal      = {USSR Computational Mathematics and Mathematical Physics},
  volume       = {4},
  number       = {5},
  pages        = {1--17},
  year         = {1964},
  doi          = {10.1016/0041-5553(64)90137-5},
}
@article{nesterov1983method,
  author       = {Yurii E. Nesterov},
  title        = {A Method for Solving the Convex Programming Problem with Convergence Rate $O(1/k^2)$},
  journal      = {Soviet Mathematics Doklady},
  volume       = {27},
  number       = {2},
  pages        = {372--376},
  year         = {1983},
}
@book{nocedal2006numerical,
  author    = {Jorge Nocedal and Stephen J. Wright},
  title     = {Numerical Optimization},
  edition   = {2nd},
  series    = {Springer Series in Operations Research and Financial Engineering},
  publisher = {Springer},
  address   = {New York, NY},
  year      = {2006},
  isbn      = {978-0-387-30303-1},
  doi       = {10.1007/978-0-387-40065-5},
}
@misc{martens2020newinsightsperspectivesnatural,
      title={New insights and perspectives on the natural gradient method}, 
      author={James Martens},
      year={2020},
      eprint={1412.1193},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.1193}, 
}
@misc{amsel2025polarexpressoptimalmatrix,
      title={The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm}, 
      author={Noah Amsel and David Persson and Christopher Musco and Robert M. Gower},
      year={2025},
      eprint={2505.16932},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.16932}, 
}
@misc{dao2022flashattentionfastmemoryefficientexact,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14135}, 
}
@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@misc{bernstein2018signsgdcompressedoptimisationnonconvex,
      title={signSGD: Compressed Optimisation for Non-Convex Problems}, 
      author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2018},
      eprint={1802.04434},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.04434}, 
}

@misc{vyas2025soapimprovingstabilizingshampoo,
      title={SOAP: Improving and Stabilizing Shampoo using Adam}, 
      author={Nikhil Vyas and Depen Morwani and Rosie Zhao and Mujin Kwun and Itai Shapira and David Brandfonbrener and Lucas Janson and Sham Kakade},
      year={2025},
      eprint={2409.11321},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.11321}, 
}
@misc{martens2020optimizingneuralnetworkskroneckerfactored,
      title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature}, 
      author={James Martens and Roger Grosse},
      year={2020},
      eprint={1503.05671},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1503.05671}, 
}
@article{duchi2011adaptive,
  author    = {John C. Duchi and Elad Hazan and Yoram Singer},
  title     = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal   = {Journal of Machine Learning Research},
  volume    = {12},
  number    = {Jul},
  pages     = {2121--2159},
  year      = {2011},
}

@misc{kunstner2024heavytailedclassimbalanceadam,
      title={Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models}, 
      author={Frederik Kunstner and Robin Yadav and Alan Milligan and Mark Schmidt and Alberto Bietti},
      year={2024},
      eprint={2402.19449},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.19449}, 
}
@misc{riabinin2025gluonmakingmuon,
      title={Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)}, 
      author={Artem Riabinin and Egor Shulgin and Kaja Gruntkowska and Peter Richtárik},
      year={2025},
      eprint={2505.13416},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.13416}, 
}
@misc{pethick2025trainingdeeplearningmodels,
      title={Training Deep Learning Models with Norm-Constrained LMOs}, 
      author={Thomas Pethick and Wanyun Xie and Kimon Antonakopoulos and Zhenyu Zhu and Antonio Silveti-Falls and Volkan Cevher},
      year={2025},
      eprint={2502.07529},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.07529}, 
}
@article{hendrycks2019benchmarking, title={Benchmarking neural network robustness to common corruptions and perturbations}, author={Hendrycks, Dan and Dietterich, Thomas}, journal={arXiv preprint arXiv:1903.12261}, year={2019} }

@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}